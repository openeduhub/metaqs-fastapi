version: '3.7'

volumes:
  fastapi-data:
  pg-data:
  elastic-data:
#  redis-data:
#  redisinsight-data:

networks:
  backend:
  frontend:

services:

  nginx:
    container_name: nginx
    image: nginx:stable-alpine
    depends_on:
      - fastapi
#      - redis
#      - redisinsight
      - postgres
      - elasticsearch
      - kibana
    environment:
      - NGINX_HOST=localhost
    networks: [ frontend ]
    ports:
      - "80:80"

  fastapi:
    container_name: fastapi
    build: ./
    image: metaqs-api-fastapi
    depends_on:
#      - redis
      - postgres
      - elasticsearch
    environment:
      - API_KEY=${API_KEY}
      - PROJECT_NAME="MetaQS API"
      - API_VERSION=${API_VERSION:-v1}
      - LOG_LEVEL=info
      - ALLOWED_HOSTS=${ALLOWED_HOSTS:-*}
      - POSTGRES_HOST=postgres
      - POSTGRES_DB=oeh
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - ELASTICSEARCH_URL=${ELASTICSEARCH_URL:-http://elasticsearch:9200}
      - ELASTICSEARCH_TIMEOUT=20
    networks: [ frontend, backend ]
    volumes:
      - fastapi-data:/var/lib/fastapi/data
#    command: uvicorn app.main:app --host 0.0.0.0 --port 80 --log-level info

#  redis:
#    container_name: redis
#    image: redis:alpine
#    environment:
#      - REDIS_PASSWORD="${REDIS_PASSWORD}"
#      - REDIS_REPLICATION_MODE=master
#    networks: [ backend ]
#    volumes:
#      - redis-data:/data
#    command:
#      # Save if 100 keys are added in every 10 seconds
#      - "--save 10 100"
#      # Set password
#      - "--requirepass ${REDIS_PASSWORD}"
#
#  redisinsight: # redis db visualization dashboard
#    container_name: redisinsight
#    image: redislabs/redisinsight
#    depends_on:
#      - redis
#    networks: [ frontend, backend ]
#    volumes:
#      - redisinsight-data:/db

  postgres:
    container_name: postgres
    image: postgres:13
    environment:
      - POSTGRES_DB=oeh
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    networks: [ backend ]
    volumes:
      - pg-data:/var/lib/postgresql/data

  elasticsearch:
    container_name: elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:7.13.4
    ulimits:
      memlock:
        hard: -1
        soft: -1
    environment:
      - xpack.security.enabled=false
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
    networks: [ backend ]
    volumes:
      - elastic-data:/usr/share/elasticsearch/data

  kibana:
    container_name: kibana
    image: docker.elastic.co/kibana/kibana:7.13.4
    environment:
      - ELASTICSEARCH_HOSTS=["${ELASTICSEARCH_URL:-http://elasticsearch:9200}"]
    depends_on:
      - elasticsearch
    networks: [ frontend, backend ]

#  superset:
#    container_name: superset
#    build:
#      context: ./
#      dockerfile: Dockerfile.apache-superset
#    image: edu-sharing/superset
#    depends_on:
#      - elasticsearch
#    networks: [ frontend, backend ]

#  importer:
#    container_name: importer
#    image: elasticdump/elasticsearch-dump
#    depends_on:
#      - elasticsearch
#    networks: [ backend ]
#    volumes:
#      - "${PWD}/wlo.json.gz:/tmp/dump.json.gz"
#    command:
#      - /bin/bash
#      - -c
#      - |
#        apt-get update \
#        && apt-get install -y curl \
#        && curl -X PUT -H "Content-Type: application/json" -d '{"index_patterns" : ["workspace"],"order" : 1,"settings" : {"index.mapping.total_fields.limit" : "2000"}}' http://elasticsearch:9200/_template/workspace 2>/dev/null | grep -q '{"acknowledged":true}' \
#        && elasticdump --input=/tmp/dump.json.gz --output=http://elasticsearch:9200/workspace --fsCompress
